{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5tTyjG5jjx0",
        "outputId": "7150c99b-7bd7-48af-85ca-f85fa4191e72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#Bayesian'\n",
            "\n",
            "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install keras-tuner #Bayesian Optimization tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Km-9N-ZdMsgN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import nrrlib as ntl\n",
        "from keras.backend import dropout\n",
        "start_time = time.time()\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random as rn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D-ew3dDfMvlQ"
      },
      "outputs": [],
      "source": [
        "# random state\n",
        "sd = 5\n",
        "np.random.seed(sd)\n",
        "rn.seed(sd)\n",
        "os.environ['PYTHONHASHSEED']=str(sd)\n",
        "tf.random.set_seed(sd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nRrKc65DiV_I"
      },
      "outputs": [],
      "source": [
        "# load das libs\n",
        "# load das libs\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, Flatten, MaxPooling1D, LSTM, Bidirectional\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop \n",
        "from gensim.models import KeyedVectors\n",
        "import sklearn.gaussian_process as gp\n",
        "import numpy\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from keras.callbacks import Callback, ModelCheckpoint, CSVLogger\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
        "import json\n",
        "\n",
        "import keras_tuner as kt #Bayesian Optimization tuner - hp [hiperparameters]\n",
        "\n",
        "\n",
        "class RocAucEvaluation(Callback):\n",
        "    def __init__(self, validation_data=(), interval=1):\n",
        "        super(Callback, self).__init__()\n",
        "\n",
        "        self.interval = interval\n",
        "        self.X_val, self.y_val = validation_data\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        if epoch % self.interval == 0:\n",
        "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
        "            self.score = roc_auc_score(self.y_val, y_pred)\n",
        "            self.false_positive_rate, self.true_positive_rate, _ = roc_curve(self.y_val, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKhPzVshigFg",
        "outputId": "41879e5a-24f8-473b-dcff-d762b54fc2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method NDFrame.head of                                                    tweet        label\n",
            "0      a primeira vez que um genocida presta solidari...      hatetag\n",
            "1                      bolsonaro assassino de  mil vidas      hatetag\n",
            "2      bolsonaro bosta bolsomerda bozolixo em realeng...      hatetag\n",
            "3      bolsonaro nazista bozolixo mito mico lulalivre...      hatetag\n",
            "4      bolsonaro nazista brasil fascismo nazismo bols...      hatetag\n",
            "...                                                  ...          ...\n",
            "31618  única coisa que atraio é os carros na hora que...  not_hatetag\n",
            "31619                         única explicação plausível  not_hatetag\n",
            "31620  única foto que tenho de ontem é uma minha e da...  not_hatetag\n",
            "31621                 único  que eu respeito bozo elenão      hatetag\n",
            "31622  único posicionamento possível da anitta neste ...  not_hatetag\n",
            "\n",
            "[31623 rows x 2 columns]>\n",
            "Dataset Loaded\n",
            "Data Preprocessed\n",
            "Max sentence length: 100\n",
            "Vocabulary size: 39618\n",
            "y é isso aqui: [1 1 1 ... 0 1 0]\n",
            "\n",
            "Time to load model: 111.2 s.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "reviews = pd.read_csv('C:/Users/Diogo/Documents/faculdade/tcc/hatetag_not_hatetag.csv')\n",
        "\n",
        "#print(reviews1.head)\n",
        "print(reviews.head)\n",
        "\n",
        "pd.set_option('display.max_colwidth',1000)\n",
        "\n",
        "classe = [1 if each == 'hatetag' else 0 for each in reviews[' label']]\n",
        "texto = reviews['tweet']\n",
        "\n",
        "print('Dataset Loaded')\n",
        "#test_tokenize(tokenize)\n",
        "text_tokenized, text_tokenizer = ntl.tokenize(texto)\n",
        "\n",
        "ntl.test_pad(ntl.pad)\n",
        "test_pad = ntl.pad(text_tokenized)\n",
        "\n",
        "target = classe\n",
        "#y = pd.get_dummies(target).values\n",
        "y = numpy.array(target)\n",
        "\n",
        "preproc_texto, texto_tokenizer =\\\n",
        "    ntl.preprocess(texto)\n",
        "    \n",
        "max_text_length = preproc_texto.shape[1]\n",
        "text_vocab_size = len(texto_tokenizer.word_index)\n",
        "\n",
        "worddx = texto_tokenizer.word_index\n",
        "\n",
        "print('Data Preprocessed')\n",
        "print(\"Max sentence length:\", max_text_length)\n",
        "print(\"Vocabulary size:\", text_vocab_size)\n",
        "\n",
        "# parametros:\n",
        "tam_vocab = text_vocab_size # vocabulário do embedding\n",
        "tam_max = max_text_length # 100\n",
        "batchsize = 128  # 64\n",
        "embedding_dimen = 300  # 50 ! 300\n",
        "filtros = 250 #250\n",
        "kernel_size = 2 # 3 | 2\n",
        "hidden_dims = 100 # not used\n",
        "epochs = 20\n",
        "lr = 0.0006 # 0.00006\n",
        "drp = 0.4   # coeficiente de dropout\n",
        "\n",
        "print(f'y é isso aqui: {y}')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(test_pad, y, test_size=0.3)\n",
        "\n",
        "#lr_schedule = optimizers.schedules.ExponentialDecay(\n",
        "#    initial_learning_rate=lr,\n",
        "#    decay_steps=10000,\n",
        "#    decay_rate=0.9)\n",
        "#optimizer = Adam(learning_rate=lr_schedule)\n",
        "#optimizer = SGD(learning_rate=lr_schedule)\n",
        "#optimizer = Nadam(learning_rate=0.01)\n",
        "\n",
        "start_time2 = time.time()\n",
        "wvec = KeyedVectors.load_word2vec_format('C:/Users/Diogo/Documents/faculdade/tcc/cbow_s300.txt')\n",
        "#wvec = KeyedVectors.load_word2vec_format(\"cbow_s50.txt\")\n",
        "#wvec = KeyedVectors.load_word2vec_format(\"skip_s50.txt\")\n",
        "print(); print(\"Time to load model: %.5s s.\\n\" % (time.time() - start_time2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ieKB4h1ai30b"
      },
      "outputs": [],
      "source": [
        "# gerando a matriz do embedding\n",
        "embedding_matrix = np.zeros((tam_vocab+1, embedding_dimen))\n",
        "for word, i in worddx.items():\n",
        "    if i>=tam_vocab:\n",
        "        continue\n",
        "    try:\n",
        "        embedding_vector = wvec[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),embedding_dimen)\n",
        "\n",
        "\n",
        "del(wvec)\n",
        "\n",
        "# defininindo a camada de embedding utilizando a matriz\n",
        "embedding_layer = Embedding(input_dim=embedding_matrix.shape[0],\n",
        "                            output_dim=embedding_matrix.shape[1],\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=tam_max,\n",
        "                            trainable=False)\n",
        "\n",
        "\n",
        "\n",
        "del(embedding_matrix)\n",
        "\n",
        "#    embedding_vec = embeddings_index.get(word)\n",
        "#    if embedding_vec is not None:\n",
        "#        embedding_matrix[index] = embedding_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxNdO2PW2E5a",
        "outputId": "9e30bb4a-ca23-47ac-f454-aa269b753538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project .\\tuning-cnn\\oracle.json\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "INFO:tensorflow:Reloading Tuner from .\\tuning-cnn\\tuner0.json\n",
            "(22136, 100)\n",
            "(9487, 100)\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "\n",
            "Tempo de execução: 126.8 segundos.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def build_model(hp):\n",
        "  model = Sequential()\n",
        "  # primeira camada se trata do embedding. \n",
        "  model.add(embedding_layer)\n",
        "\n",
        "  # input = numero de possiveis palavras, dimensão do espaço(complex!), e 137\n",
        "  #model.add(Embedding(tam_vocab, embedding_dimen, input_length=tam_max)) \n",
        "\n",
        "  # camada convolucional\n",
        "  # filtros referentes ao output dessa camada,\n",
        "  # tamanho nucleo = 3, se tratando apenas de uma dimensão (conv1d)\n",
        "  # padding 'valid' = sem padding\n",
        "  # função de ativação = relu\n",
        "\n",
        "  for i in range(hp.Int('num_blocks', 1, 2)):\n",
        "      #hp_padding = hp.Choice('padding_'+ str(i), values=['valid', 'same'])\n",
        "      #hp_filters = hp.Choice('filters_'+ str(i), values=[32, 64, 250])\n",
        "\n",
        "      #model.add(Conv1D(hp_filters, kernel_size, padding=hp_padding, activation='relu', kernel_initializer='he_uniform', strides=1))\n",
        "      #model.add(Conv1D(hp_filters, kernel_size, padding=hp_padding, activation='relu', kernel_initializer='he_uniform', strides=1))\n",
        "      model.add(Bidirectional(LSTM(50, activation= 'relu', return_sequences=True)))\n",
        "      model.add(Dropout(hp.Choice('dropout_'+ str(i), values=[0.0, 0.1, 0.2, 0.4])))\n",
        "  \n",
        "  model.add(Bidirectional(LSTM(50, activation= 'relu')))\n",
        "  model.add(Dropout(hp.Choice('dropout_'+ str(i), values=[0.0, 0.1, 0.2, 0.4])))\n",
        "\n",
        "\n",
        "  # nivela o output para utilização na camada densa\n",
        "  model.add(Flatten())\n",
        "\n",
        "  #model.add(Dense(2, activation='sigmoid')) #FUNÇÃO VAGABUNDA\n",
        "  model.add(Dense(1, activation='sigmoid')) \n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3])\n",
        "  hp_optimizer = hp.Choice('optimizer', values=['adam', 'nadam', 'rmsprop'])\n",
        "\n",
        "  '''\n",
        "  if hp_optimizer == 'Adam':\n",
        "    learning_rate = hp_learning_rate\n",
        "  elif hp_optimizer == 'SGD':\n",
        "    learning_rate = hp_learning_rate\n",
        "    nesterov=True\n",
        "    momentum=0.9\n",
        "  elif hp_optimizer == 'Optimizer':\n",
        "    hp_optimizer = optimizer'''\n",
        "\n",
        "\n",
        "\n",
        "  # calculo de loss adotado = binary_crossentropy/categorical_crossentropy\n",
        "  #model.compile(loss='binary_crossentropy', optimizer=Adam(lr), metrics=['accuracy'])\n",
        "  model.compile(loss='binary_crossentropy', optimizer=hp_optimizer, metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "\n",
        "tuner_cnn = kt.tuners.BayesianOptimization(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=10,\n",
        "    directory='.',\n",
        "    project_name='tuning-cnn')\n",
        "\n",
        "\n",
        "print(X_train.shape)\n",
        "#print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "#print(y_test.shape)\n",
        "\n",
        "# Fit the model                                                                                               #val split\n",
        "# history = model.fit(X_train, y_train, batch_size=batchsize, epochs=epochs,validation_data=(X_test, y_test), validation_split = 0.3, verbose=2)\n",
        "tuner_cnn.search(X_train, y_train, batch_size=batchsize, epochs=epochs, validation_data=(X_test, y_test), validation_split = 0.3, verbose=2)\n",
        "\n",
        "# Final evaluation of the model\n",
        "# scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "#print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# tempo de execução total\n",
        "print(); print(\"Tempo de execução: %.5s segundos.\\n\" % (time.time() - start_time))\n",
        "\n",
        "# plot dos gráficos\n",
        "# ntl.plot_accuracy(history)\n",
        "# ntl.plot_loss(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0pTJsWyWuz_u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Modelo ideal:\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'num_blocks': 1,\n",
              " 'dropout_0': 0.0,\n",
              " 'learning_rate': 0.01,\n",
              " 'optimizer': 'rmsprop',\n",
              " 'dropout_1': 0.4}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_model = tuner_cnn.get_best_models()[0]\n",
        "best_hp = tuner_cnn.get_best_hyperparameters(1)[0]\n",
        "print('Modelo ideal:\\n')\n",
        "best_hp.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sAXpmUgMDAY1"
      },
      "outputs": [],
      "source": [
        "best_acc = tuner_cnn.oracle.get_best_trials()[0].score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "U6n4o4R3Cj6v"
      },
      "outputs": [],
      "source": [
        "best_hpp = tuner_cnn.get_best_hyperparameters(1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Epoch 1/20\n",
            "173/173 - 309s - loss: 2641034477568.0000 - accuracy: 0.8489 - val_loss: 60058910720.0000 - val_accuracy: 0.7986 - 309s/epoch - 2s/step\n",
            "Epoch 2/20\n",
            "173/173 - 300s - loss: 2492504473600.0000 - accuracy: 0.8516 - val_loss: 0.4629 - val_accuracy: 0.8620 - 300s/epoch - 2s/step\n",
            "Epoch 3/20\n",
            "173/173 - 299s - loss: 0.4389 - accuracy: 0.8673 - val_loss: 0.4174 - val_accuracy: 0.8754 - 299s/epoch - 2s/step\n",
            "Epoch 4/20\n",
            "173/173 - 299s - loss: 0.3609 - accuracy: 0.8943 - val_loss: 0.3031 - val_accuracy: 0.9105 - 299s/epoch - 2s/step\n",
            "Epoch 5/20\n",
            "173/173 - 300s - loss: 0.2814 - accuracy: 0.9132 - val_loss: 0.2326 - val_accuracy: 0.9225 - 300s/epoch - 2s/step\n",
            "Epoch 6/20\n",
            "173/173 - 299s - loss: 115714456.0000 - accuracy: 0.8009 - val_loss: 3112.3652 - val_accuracy: 0.2572 - 299s/epoch - 2s/step\n",
            "Epoch 7/20\n",
            "173/173 - 299s - loss: 2240.5510 - accuracy: 0.2742 - val_loss: 702.7675 - val_accuracy: 0.1427 - 299s/epoch - 2s/step\n",
            "Epoch 8/20\n",
            "173/173 - 299s - loss: 117.9573 - accuracy: 0.4531 - val_loss: 0.2342 - val_accuracy: 0.9296 - 299s/epoch - 2s/step\n",
            "Epoch 9/20\n",
            "173/173 - 298s - loss: 1.2330 - accuracy: 0.9345 - val_loss: 0.1670 - val_accuracy: 0.9448 - 298s/epoch - 2s/step\n",
            "Epoch 10/20\n",
            "173/173 - 298s - loss: 7.5138 - accuracy: 0.9391 - val_loss: 160.9140 - val_accuracy: 0.5711 - 298s/epoch - 2s/step\n",
            "Epoch 11/20\n",
            "173/173 - 336s - loss: 216.3906 - accuracy: 0.6786 - val_loss: 161.2474 - val_accuracy: 0.1379 - 336s/epoch - 2s/step\n",
            "Epoch 12/20\n",
            "173/173 - 320s - loss: 186560784.0000 - accuracy: 0.6944 - val_loss: 2751.9053 - val_accuracy: 0.8592 - 320s/epoch - 2s/step\n",
            "Epoch 13/20\n",
            "173/173 - 308s - loss: 147519.4844 - accuracy: 0.8804 - val_loss: 252.0865 - val_accuracy: 0.9134 - 308s/epoch - 2s/step\n",
            "Epoch 14/20\n",
            "173/173 - 297s - loss: 3515.7056 - accuracy: 0.9419 - val_loss: 0.1586 - val_accuracy: 0.9497 - 297s/epoch - 2s/step\n",
            "Epoch 15/20\n",
            "173/173 - 311s - loss: 1.1898 - accuracy: 0.9482 - val_loss: 0.1622 - val_accuracy: 0.9463 - 311s/epoch - 2s/step\n",
            "Epoch 16/20\n",
            "173/173 - 297s - loss: 0.1666 - accuracy: 0.9554 - val_loss: 0.1284 - val_accuracy: 0.9589 - 297s/epoch - 2s/step\n",
            "Epoch 17/20\n",
            "173/173 - 298s - loss: 0.1126 - accuracy: 0.9661 - val_loss: 0.1179 - val_accuracy: 0.9610 - 298s/epoch - 2s/step\n",
            "Epoch 18/20\n",
            "173/173 - 297s - loss: 0.0932 - accuracy: 0.9692 - val_loss: 0.1144 - val_accuracy: 0.9623 - 297s/epoch - 2s/step\n",
            "Epoch 19/20\n",
            "173/173 - 299s - loss: 283244.1875 - accuracy: 0.9706 - val_loss: 3427.5171 - val_accuracy: 0.9540 - 299s/epoch - 2s/step\n",
            "Epoch 20/20\n",
            "173/173 - 296s - loss: 314151136.0000 - accuracy: 0.8987 - val_loss: 26404.1660 - val_accuracy: 0.9185 - 296s/epoch - 2s/step\n",
            "297/297 [==============================] - 74s 248ms/step - loss: 26404.1504 - accuracy: 0.9185\n",
            "297/297 [==============================] - 72s 242ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Diogo\\AppData\\Local\\Temp\\ipykernel_9504\\3926560258.py:61: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  resultados = resultados.append(registro_resultado, ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "resultados = pd.read_csv('../resultados.csv')\n",
        "\n",
        "opt_model = tuner_cnn.hypermodel.build(best_hpp)\n",
        "\n",
        "# ra_val = RocAucEvaluation(validation_data=(X_test, y_test), interval = 1)\n",
        "\n",
        "# csv_logger = CSVLogger('log.csv', append=False, separator=';')\n",
        "\n",
        "# history = opt_model.fit(X_train, y_train, batch_size=batchsize, epochs=epochs,validation_data=(X_test, y_test), callbacks=[EarlyStopping(monitor='val_loss', patience=8),\n",
        "#              ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True), ra_val, csv_logger], validation_split = 0.3, verbose=2)\n",
        "\n",
        "\n",
        "# scores = opt_model.evaluate(X_test, y_test, verbose=1)\n",
        "\n",
        "# y_predito = opt_model.predict(X_test).round()  \n",
        "# y_gabarito = y_test\n",
        "\n",
        "# # Parâmetros\n",
        "# # rede base otimizador acuracia val_loss learning_rate dropout batch_size roc_curve train_resume confusion_matrix\n",
        "# NOME_REDE = \"V6\"\n",
        "# NOME_BASE = 'PADRAO'\n",
        "# NOME_OTIMIZADOR = tuner_cnn.get_best_hyperparameters(1)[0]['optimizer']\n",
        "# ACURACIA = scores[1]\n",
        "# LOSS = scores[0] \n",
        "# LEARNING_RATE = tuner_cnn.get_best_hyperparameters(1)[0]['learning_rate']\n",
        "# DROPOUT_RATE = tuner_cnn.get_best_hyperparameters(1)[0]['dropout_1']\n",
        "# BATCH = batchsize\n",
        "# ROC_CURVE = json.dumps({\n",
        "#     \"false_positive_rate\": list(ra_val.false_positive_rate),\n",
        "#     \"true_positive_rate\": list(ra_val.true_positive_rate),\n",
        "#     \"score\": ra_val.score})\n",
        "# TRAIN_RESUME = json.dumps(pd.read_csv('log.csv',sep=';').to_dict()) \n",
        "\n",
        "# confusion = confusion_matrix(y_gabarito, y_predito)\n",
        "# CONFUSION_MATRIX = json.dumps({\n",
        "#     \"00\": int(confusion[0][0]),\n",
        "#     \"01\": int(confusion[0][1]),\n",
        "#     \"10\": int(confusion[1][0]),\n",
        "#     \"11\": int(confusion[1][1])\n",
        "# })\n",
        "\n",
        "# EPOCHS = len(history.history['val_loss'])\n",
        "# RANK = ACURACIA/LOSS\n",
        "\n",
        "# # Salvando resultado do modelo\n",
        "# registro_resultado = {resultados.columns[0]: NOME_REDE, \n",
        "#                         resultados.columns[1]: NOME_BASE, \n",
        "#                         resultados.columns[2]: NOME_OTIMIZADOR,\n",
        "#                         resultados.columns[3]: ACURACIA, \n",
        "#                         resultados.columns[4]: LOSS,\n",
        "#                         resultados.columns[5]: LEARNING_RATE,\n",
        "#                         resultados.columns[6]: DROPOUT_RATE,\n",
        "#                         resultados.columns[7]: BATCH,\n",
        "#                         resultados.columns[8]: ROC_CURVE,\n",
        "#                         resultados.columns[9]: TRAIN_RESUME,\n",
        "#                         resultados.columns[10]: CONFUSION_MATRIX,\n",
        "#                         resultados.columns[11]: EPOCHS,\n",
        "#                         resultados.columns[12]: RANK,\n",
        "#                     }\n",
        "\n",
        "# resultados = resultados.append(registro_resultado, ignore_index=True)\n",
        "\n",
        "# resultados.to_csv('../resultados.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rLJJE4Y4CEa-"
      },
      "outputs": [],
      "source": [
        "#opt_model = tuner_cnn.hypermodel.build(best_hpp)\n",
        "#history = opt_model.fit(X_train, y_train, batch_size=batchsize, epochs=epochs*3,validation_data=(X_test, y_test), validation_split = 0.3, verbose=1, callbacks = [EarlyStopping(monitor='val_loss', patience=6),\n",
        "#             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FjbaIl1kC_J9"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Final evaluation of the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m scores \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAccuracy: \u001b[39m\u001b[39m%.2f\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (scores[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[39m# plot dos gráficos\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Final evaluation of the model\n",
        "import pydot\n",
        "\n",
        "tf.keras.utils.plot_model(opt_model, to_file='v6.png', show_shapes=True)\n",
        "# scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "# print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "# # plot dos gráficos\n",
        "# ntl.plot_accuracy(history)\n",
        "# ntl.plot_loss(history)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "9403936ddbb94aea0a9f331f9ab296deb47ae18a255e6907c0bae275c12cd30e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
